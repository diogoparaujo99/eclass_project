\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
\PassOptionsToPackage{square,numbers}{natbib}

% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{enumitem}

%\usepackage[square,numbers]{natbib}
%\bibliographystyle{abbrvnat}
\bibliographystyle{unsrtnat}



\title{Multi-modal Brain Tumor Segmentation \\ \small DSL Project Proposal}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Diogo Araújo \\
  \texttt{ist193906} \\
  \And
  Gonçalo Matos\\
  \texttt{ist175150} \\
  % examples of more authors
   \And
   Marc Golub \\
   \texttt{ist176084} \\
   \AND
   Sofia Monteiro \\
   \texttt{ist186751} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


%\begin{abstract}
%  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
%  both the left- and right-hand margins. Use 10~point type, with a vertical
%  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
%  bold, and in point size 12. Two line spaces precede the abstract. The abstract
%  must be limited to one paragraph.
%\end{abstract}


\subsection{Greedy Neighborhood-Aware MAP from Filtering Marginals}

Let $x_t \in \{0,1,\dots,N-1\}$ denote the hidden state (grid cell id) at time $t$, and let
\[
\boldsymbol{\pi}_t \in \mathbb{R}^N,\qquad \pi_t(i)=\Pr(x_t=i \mid o_{1:t})
\]
be the \emph{filtering marginal} at time $t$ (in code: \texttt{probs\_history[t-1]}).

Let $A \in \mathbb{R}^{N\times N}$ be the transition matrix with entries
$A_{ij}=\Pr(x_t=j \mid x_{t-1}=i)$.
Define the set of valid \emph{predecessors} of a state $j$ by
\[
\mathcal{N}^{-}(j) \;=\; \{\, i \in \{0,\dots,N-1\} \;:\; A_{ij} > 0 \,\}.
\]
Equivalently, define a binary predecessor mask vector for state $j$:
\[
\mathbf{m}(j) \in \{0,1\}^N,\qquad m_i(j)=\mathbb{I}[A_{ij}>0].
\]

\paragraph{Objective and Motivation.}
A full MAP estimate of the entire trajectory $\hat{x}_{1:T}$ under an HMM posterior is obtained by Viterbi decoding. Here we instead construct a \emph{greedy} path that is (i) consistent with the transition graph (grid neighborhood), and (ii) prefers states with high filtering marginals at each time. This yields an inexpensive, neighborhood-aware trajectory estimate derived from $\{\boldsymbol{\pi}_t\}_{t=1}^T$.

\paragraph{Greedy Backtracking Rule.}
We initialize the terminal state as the most probable marginal at time $T$:
\[
\hat{x}_T \;=\; \arg\max_{i \in \{0,\dots,N-1\}} \pi_T(i).
\]
Then for $t=T-1,T-2,\dots,1$ we choose the predecessor that maximizes the previous-time marginal
among states that can transition into the already chosen successor:
\[
\hat{x}_t
\;=\;
\arg\max_{i \in \mathcal{N}^{-}(\hat{x}_{t+1})} \pi_t(i).
\]
In masked-vector form, this can be written as
\[
\hat{x}_t
\;=\;
\arg\max_{i \in \{0,\dots,N-1\}}
\bigl(\pi_t(i)\, m_i(\hat{x}_{t+1})\bigr),
\]
i.e., an elementwise product between the marginal vector and the predecessor mask, followed by an $\arg\max$.

\paragraph{Fallback (Optional).}
If $\mathcal{N}^{-}(\hat{x}_{t+1})$ is empty (unexpected in a well-formed grid), one can fall back to the unconstrained marginal mode:
\[
\hat{x}_t \leftarrow \arg\max_i \pi_t(i).
\]

\paragraph{Remarks.}
This method produces a transition-consistent trajectory while leveraging the filtering marginals. It is \emph{not} equivalent to the true MAP trajectory $\arg\max_{x_{1:T}}\Pr(x_{1:T}\mid o_{1:T})$ (which Viterbi computes), because it greedily optimizes each step under a local neighborhood constraint rather than optimizing the full sequence probability.


\section{Problem description}\label{sec:intro}

Manual \textbf{segmentation} of brain tumors in Magnetic Resonance Imaging (MRI) is time consuming and labor-intensive. Neural networks --- namely the U-net architecture \cite{henry2021brain} -- have been instrumental for the development of automatic segmentation tools.  These networks are often trained on multiple imaging modalities, which correspond to different scanner settings in which the brain can be imaged. Each modality makes certain regions of the tumors more easily detectable than others. However, sometimes not all of the modalities are available for a given patient, limiting the applicability of these tools \cite{wang2023multimodal}. Acquiring multiple modalities is costly and time consuming, and the contrast injections that are necessary to acquire some of the modalities are contraindicated in certain patients (for example, patients with severe kidney failure) \cite{zhang2021toward}.

To address this challenge, we propose a deep learning framework capable of adapting to \textbf{missing} data for some of the \textbf{modalities}, that is, a neural network that can be used to make inferences either with all or only a subset of the modalities available.
This will be a \textbf{practical} project where, in the end, we would like to answer the following research questions:
% explore the possibility of training a neural network on multi-modal data in such a way that it can be used to make inferences either with all or only a subset of the modalities available.

\begin{itemize}
    \item Can we train a single unified model that is able to process inputs with a variable number (1--4) of modalities?
    \item Is it possible to drop some of the modalities during inference without sacrificing the performance too much? Which one(s)?
\end{itemize}


\section{Data}

We will use the \textbf{BraTS 2021 public dataset} \cite{brats2021_website}, which is a multi-modal dataset comprising 1251 patients with four MRI modalities and ground truth segmentation labels offered by experts.
%For each patient, four MRI modalities are provided: native (T1), post-contrast T1-weighted (T1Gd), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (T2-FLAIR). 
%It is available at \url{https://www.med.upenn.edu/cbica/brats2021}.


\section{Methodology}

We plan to address this problem with the following incremental approach:

\begin{enumerate}[label=\Roman*.]
    \item Implement a U-Net architecture \cite{u-net} for tumor segmentation, following the methodology outlined in \cite{henry2021brain}. This model will serve as our baseline;\label{item:u-net_baseline}
    \item Develop and implement a U-Net-based framework which handles missing modalities;\label{item:proposed_approach}
    \item Explore the use of an architecture based on Vision Transformers \cite{vit}, which are capable of handling variable input lengths;\label{item:future_work}
\end{enumerate}

The segmentation performance of the models in \ref{item:proposed_approach} and \ref{item:future_work} will be compared against the baseline U-Net model in \ref{item:u-net_baseline}, using standard \textbf{metrics} for semantic segmentation tasks, such as Dice coefficient and Haussdorff distance \cite{henry2021brain}.
 %, Precision, and Recall.Ultimately, we want to find out the answers for the questions raised in the \ref{sec:intro}.

% We intend to implement the U-net model as described in \cite{henry2021brain} and run it over our dataset, to serve as a baseline to later compare with our own models. Then, we will evaluate the results achieved by the models in 2. and 3. by comparing them with the baseline 1., using the metrics \textit{Dice loss}, \textit{Precision} and \textit{Recall}.



\medskip

\bibliography{bibliography}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}