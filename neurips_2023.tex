\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
\PassOptionsToPackage{square,numbers}{natbib}

% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{graphicx}       % images
\usepackage{amsmath}        % math
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{enumitem}

%\usepackage[square,numbers]{natbib}
%\bibliographystyle{abbrvnat}
\bibliographystyle{unsrtnat}



\title{Robot Self-localization}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Diogo Ara√∫jo \\
  \texttt{ist193906} \\
  \And
  Jorge Cerejo \\
  \texttt{ist189815} \\
}


\begin{document}


\maketitle


%\begin{abstract}
%  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
%  both the left- and right-hand margins. Use 10~point type, with a vertical
%  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
%  bold, and in point size 12. Two line spaces precede the abstract. The abstract
%  must be limited to one paragraph.
%\end{abstract}


\section{Introduction}
This project was implemented in the scope of the graduate course of Estimation and Classification.
We model robot self-localization in a GridWorld as a Hidden Markov Model (HMM): Section~1 defines the state space, initial distribution, transition model, and observation model; Section~2 describes the online filtering recursion; Section~3 compares Viterbi decoding against a neighbor-restricted MAP-from-marginals heuristic; Section~4 concludes and outlines future work.

\section{HMM Formulation}
We consider an $N\times N$ grid map and define a discrete state $x_t$ as the robot cell index, with state-space size $|\mathcal{S}|=N^2$.
At each time step, the robot receives a 4-bit binary observation $o_t=(o^N,o^E,o^S,o^W)\in\{0,1\}^4$, where $0$ indicates an obstacle/boundary in that direction and $1$ indicates a free direction.

\subsection{Initial Distribution}
For global localization we use a uniform prior over all states, $\pi(i)=P(x_1=i)=1/N^2$.

\subsection{Transition Model}
The motion model is a random walk with transition matrix $T$, where
$T_{ij}=P(x_t=j\mid x_{t-1}=i)$ is uniform over the valid neighboring cells (e.g., in corridors/borders a cell can have three valid neighbors).

\subsection{Observation Model}
The emission matrix $O$ has 16 symbols ($2^4$ possible 4-bit observations), with entries $O_{i,z}=P(o_t=z\mid x_t=i)$.
We use an independent bit-flip sensor model with error probability $p_e$; letting $d_H(\cdot,\cdot)$ be the Hamming distance between the observed pattern and the deterministic map signature at state $i$,
\[
P(o_t\mid x_t=i)=(1-p_e)^{4-d_H}\,p_e^{d_H}.
\]

\section{Filtering for Self-localization}
Self-localization is a filtering problem, computing $P(x_t\mid o_{1:t})$ online.
Let $p_t\in\mathbb{R}^{N^2}$ be the belief vector at time $t$ and let $b(o_t)\in\mathbb{R}^{N^2}$ be the emission-likelihood vector for $o_t$ (the corresponding column of $O$). The recursion is:
\[
\hat{p}_t=T^\top p_{t-1},\qquad
p_t \propto b(o_t)\odot \hat{p}_t,
\]
followed by normalization.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{5pt}
\begin{tabular}{lccc}
\toprule
Map size & $p_e=0.0$ & $p_e=0.05$ & $p_e=0.4$ \\
\midrule
$20\times20$ & 1.8/1.7 & 2.4/2.2 & 11.4/9.6 \\
$25\times25$ & 3.6/2.7 & 8.4/7.0 & 18.6/14.7 \\
$30\times30$ & 3.2/2.6 & 5.4/4.6 & 16.4/12.9 \\
\bottomrule
\end{tabular}
\caption{Effect of sensor error $p_e$ on final-step localization error. Each entry reports mean $L_1/L_2$ distance (Manhattan/Euclidean) between the true final position and the estimate $\arg\max p_T$ (mean over 5 runs; obstacle ratio 0.2; horizon 100).}
\label{tab:pe-error}
\end{table}

As expected, larger $p_e$ degrades the final position estimate.
Even when filtering remains stable, the argmax estimate becomes less informative as observation likelihoods flatten with higher noise.

\section{Most Likely State Sequence}
Beyond pointwise filtering, we estimated a full trajectory $\hat{x}_{1:T}$.
Viterbi decoding computes the most likely sequence under the HMM dynamics using a forward recursion (online $\delta_t$ and backpointers $\psi_t$) and a final backtrack.
In addition, we implemented a greedy neighbor-restricted MAP-from-marginals path: starting from $\arg\max p_T$, it backtracks by selecting at each step the highest-probability predecessor among valid incoming neighbors in $T$.

Empirically, the greedy MAP-from-marginals can appear more robust than Viterbi at high sensor noise.
A plausible explanation is that high noise increases belief entropy and, with near-uniform transitions, many sequences have similar joint probability; in this regime, Viterbi tie-breaking in $\delta/\psi$ can lead to arbitrary path choices.

\begin{figure*}[t]
\centering
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.85\linewidth]{figures/viterbi_path.png}\\
\small (a) Viterbi (red)
\end{minipage}\hfill
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.85\linewidth]{figures/map_path.png}\\
\small (b) Greedy MAP-from-marginals (green)
\end{minipage}
\caption{Example state-sequence estimates on the same episode.}
\label{fig:paths}
\end{figure*}

\section{Conclusion}
We formulated robot self-localization as an HMM on a GridWorld and implemented the standard filtering recursion to maintain $P(x_t\mid o_{1:t})$ online; the table in Section~2 illustrates the expected impact of sensor noise on final-step accuracy.
We also explored sequence estimation via Viterbi decoding and a neighbor-restricted MAP-from-marginals heuristic, noting that trajectory estimation is challenging when transitions are close to uniform and observations are noisy, which can make Viterbi sensitive to near-ties.
Future work includes non-uniform priors, action-conditioned transition models, and controlled settings (e.g., simple pursuit dynamics as in a drone tracking scenario).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
